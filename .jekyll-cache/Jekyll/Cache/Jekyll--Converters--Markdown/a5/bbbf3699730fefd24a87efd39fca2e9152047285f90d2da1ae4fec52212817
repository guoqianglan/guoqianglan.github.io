I"„2<p>In this post, I will show you how to deploy Heketi-Glusterfs on Kubernetes cluster for dynamic storage.
Here I assumed that you already have a Kubernetes cluster,
if not, please follow my <a href="/deploy-kubernetes-with-kubespray-on-openstack">previous blog</a> to build one.</p>

<h2 id="cluster-status">Cluster Status</h2>

<p>Master nodes:</p>
<ul>
  <li>k8s-cluster-k8s-master-1 192.168.147.6</li>
</ul>

<p>Worker nodes:</p>
<ul>
  <li>k8s-cluster-k8s-node-nf-1 192.168.147.13 /dev/vdb</li>
  <li>k8s-cluster-k8s-node-nf-2 192.168.147.17 /dev/vdb</li>
  <li>k8s-cluster-k8s-node-nf-3 192.168.147.25 /dev/vdb</li>
  <li>k8s-cluster-k8s-node-nf-4 192.168.147.16 /dev/vdb</li>
</ul>

<p><img src="/images/blog_kube_jhub/volumn_setup.png" alt="_config.yml" /></p>

<p><strong>Note</strong>:</p>
<ul>
  <li>You just need to create the volumns (â€˜/dev/vdbâ€™) and attach them to the worker nodes, please donâ€™t do anything else to them yet.*</li>
</ul>

<h2 id="requirements">Requirements</h2>

<ul>
  <li>
    <p><em>Local:</em></p>

    <ol>
      <li>Create a new python virtual environment, e.g., â€˜openstack_envâ€™, activate it, and install openstackclient.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtualenv <span class="nt">--python</span><span class="o">=</span>python3 <span class="nb">env</span>/openstack_env
<span class="nb">source env</span>/openstack_env/bin/activate
pip <span class="nb">install </span>python-openstackclient
</code></pre></div>        </div>
      </li>
      <li>Install the dependencies of Kubespary.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/kubernetes-sigs/kubespray
<span class="nb">cd </span>kubespray
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div>        </div>
      </li>
      <li><a href="https://www.terraform.io/intro/getting-started/install.html">Install Terraform 0.12</a> or later.</li>
      <li>Download the OpenStack RC File from your cloud provider and load it.
<img src="/images/blog_kube_jhub/download_rc_file.png" alt="_config.yml" />
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> &lt;your_project_name&gt;-openrc.sh
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h2 id="setup-cluster">Setup cluster</h2>

<p>In kubespary directory, exectucate following commands.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">CLUSTER</span><span class="o">=</span>my-kube
  <span class="nb">cp</span> <span class="nt">-LRp</span> contrib/terraform/openstack/sample-inventory <span class="se">\</span>
  inventory/<span class="nv">$CLUSTER</span>
  <span class="nb">cd </span>inventory/<span class="nv">$CLUSTER</span>
  <span class="nb">ln</span> <span class="nt">-s</span> ../../contrib/terraform/openstack/hosts
  <span class="nb">ln</span> <span class="nt">-s</span> ../../contrib
</code></pre></div></div>
<p>Edit the cluster variable file, <code class="highlighter-rouge">inventory/$CLUSTER/cluster.tfvars</code>.</p>

<p>For example:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># your Kubernetes cluster name here</span>
<span class="s">cluster_name = "k8s-cluster"</span>

<span class="c1"># list of availability zones available in your OpenStack cluster</span>
<span class="s">az_list = ["Persistent_01", "Persistent_02"]</span>
<span class="s">az_list_node = ["Persistent_01", "Persistent_02"]</span>

<span class="s">dns_nameservers=["100.125.4.25", "8.8.8.8"]</span>

<span class="c1"># SSH key to use for access to nodes</span>
<span class="s">public_key_path = "~/.ssh/id_rsa.pub"</span>

<span class="c1"># image to use for bastion, masters, standalone etcd instances, and nodes</span>
<span class="s">image = "CentOS-7-x64-2019-07"</span>

<span class="c1"># user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)</span>
<span class="s">ssh_user = "centos"</span>

<span class="c1"># 0|1 bastion nodes</span>
<span class="s">number_of_bastions = </span><span class="m">0</span>
<span class="s">flavor_bastion = "448319d3-2417-4eb1-9da2-63a2fdbc23f6"</span>

<span class="c1"># standalone etcds</span>
<span class="s">number_of_etcd = </span><span class="m">0</span>
<span class="s">flavor_etcd = "448319d3-2417-4eb1-9da2-63a2fdbc23f6"</span>

<span class="c1"># masters</span>
<span class="s">number_of_k8s_masters = </span><span class="m">1</span>

<span class="s">number_of_k8s_masters_no_etcd = </span><span class="m">0</span>

<span class="s">number_of_k8s_masters_no_floating_ip = </span><span class="m">0</span>

<span class="s">number_of_k8s_masters_no_floating_ip_no_etcd = </span><span class="m">0</span>

<span class="s">flavor_k8s_master = "448319d3-2417-4eb1-9da2-63a2fdbc23f6"</span>

<span class="c1"># nodes</span>
<span class="s">number_of_k8s_nodes = </span><span class="m">0</span>

<span class="s">number_of_k8s_nodes_no_floating_ip = </span><span class="m">4</span>

<span class="s">flavor_k8s_node = "448319d3-2417-4eb1-9da2-63a2fdbc23f6"</span>

<span class="c1"># GlusterFS</span>
<span class="c1"># either 0 or more than one</span>
<span class="c1"># number_of_gfs_nodes_no_floating_ip = 1</span>
<span class="c1"># gfs_volume_size_in_gb = 500</span>
<span class="c1"># Container Linux does not support GlusterFS</span>
<span class="s">image_gfs = "CentOS-7-x64-2019-07"</span>
<span class="c1"># May be different from other nodes</span>
<span class="s">ssh_user_gfs = "centos"</span>
<span class="s">flavor_gfs_node = "448319d3-2417-4eb1-9da2-63a2fdbc23f6"</span>

<span class="c1"># networking</span>
<span class="s">network_name = "k8s-network"</span>

<span class="s">external_net = "6621bf61-6094-4b24-a9a0-f5794c3a881e"</span>

<span class="s">subnet_cidr = "192.168.147.0/24"</span>

<span class="s">floatingip_pool = "Public-Network"</span>

<span class="s">bastion_allowed_remote_ips = ["0.0.0.0/0"]</span>

</code></pre></div></div>
<p>To start the Terraform deployment, you need to install some plugins using command as follows.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>terraform init contrib/terraform/openstack
</code></pre></div></div>
<p>Start to build the cluster.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>terraform apply <span class="nt">-var-file</span><span class="o">=</span>cluster.tfvars ../../contrib/terraform/openstack
</code></pre></div></div>
<p>If it is finished successfully, you will get output as follows.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Apply <span class="nb">complete</span><span class="o">!</span> Resources: 5 added, 0 changed, 0 destroyed.

Outputs:

bastion_fips <span class="o">=</span> <span class="o">[]</span>
floating_network_id <span class="o">=</span> 6621bf61-<span class="k">****************</span>
k8s_master_fips <span class="o">=</span> <span class="o">[</span><span class="s2">"206.**.**.***"</span>,]
k8s_node_fips <span class="o">=</span> <span class="o">[]</span>
private_subnet_id <span class="o">=</span> dfa59b71-<span class="k">**************</span>
router_id <span class="o">=</span> cf695cfb-<span class="k">******************</span>
</code></pre></div></div>
<p>Then go back to the kubespary root directory. Try if ansible can successfully reach our clusters using</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible <span class="nt">-i</span> inventory/my-kube/hosts <span class="nt">-m</span> ping all
</code></pre></div></div>
<p>If all of the nodes are accessible, the output would look like
<img src="/images/blog_kube_jhub/ansible_ping_success.png" alt="_config.yml" /></p>

<p><strong>Note</strong>:</p>

<ul>
  <li><em>If the cluster is unreachable, please open additional TCP port 22 for SSH  then try again.</em></li>
  <li><em>There are other ports you need to open, i.e., ICMP port which enable pinging master ip externally, TCP 2379 for etcd connection</em></li>
</ul>

<p><img src="/images/blog_kube_jhub/add_additional_security_rules.png" alt="_config.yml" /></p>

<p>Then letâ€™s see how to build Kubernetes on this cluster. Some additional configurations need to be modified.</p>

<ol>
  <li>In <code class="highlighter-rouge">inventory/$CLUSTER/group_vars/all/all.yml</code>, set up
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">cloud_provider</span><span class="pi">:</span> <span class="s">openstack</span>
</code></pre></div>    </div>
  </li>
  <li>In <code class="highlighter-rouge">inventory/$CLUSTER/group_vars/k8s-cluster/k8s-cluster.yml</code>, set up
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kube_network_plugin</span><span class="pi">:</span> <span class="s">flannel</span>
<span class="na">resolvconf_mode</span><span class="pi">:</span> <span class="s">docker_dns</span>
<span class="na">use_access_ip</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div>    </div>
  </li>
  <li>In <code class="highlighter-rouge">inventory/$CLUSTER/group_vars/k8s-cluster/addons.yml</code> set up
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">helm_enabled</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div>    </div>
  </li>
</ol>

<p><strong>Note</strong>:</p>
<ul>
  <li><em>If you failed to ping float ip (<code class="highlighter-rouge">ping &lt;float_ip&gt;</code>), please open ICMP any port and then try again.</em></li>
</ul>

<p>Then make sure you have good internet connection, or you may get timeout exception when you run the ansible-playbook.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook <span class="nt">--become</span> <span class="nt">-i</span> inventory/<span class="nv">$CLUSTER</span>/hosts cluster.yml
</code></pre></div></div>
<p><strong>Note</strong>:</p>
<ul>
  <li><em>If you failed to pass the etcd cluster healthy check, you may need to open port, TCP 2379. If it still doesnâ€™t help, you would need to login the master node 
and run commamd</em>,
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chmod </span>755 <span class="nt">-R</span> /etc/ssl/etcd
</code></pre></div>    </div>
    <p><em>After that, you can check the healthy status by running</em></p>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>etcdctl --endpoints https://&lt;master_ip&gt;:2379 --ca-file=/etc/ssl/etcd/ssl/ca.pem --cert-file=/etc/ssl/etcd/ssl/member-k8s-cluster-k8s-master-1.pem --key-file=/etc/ssl/etcd/ssl/member-k8s-cluster-k8s-master-1-key.pem cluster-health
</code></pre></div>    </div>
  </li>
</ul>

<p>After all of these have been done, Please login in your master node and try to run <code class="highlighter-rouge">kubectl get nodes</code>, it should show the output like this
<img src="/images/blog_kube_jhub/kubectl_get_nodes.png" alt="_config.yml" /></p>

<p><strong>Note</strong>:</p>
<ul>
  <li><em>If it return error message, 
<code class="highlighter-rouge">"The connection to the server localhost:8080 was refused - did you specify the right host or port?"</code>, 
please try to set up $KUBECONFIG using following command,</em>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo cp</span> /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/ <span class="o">&amp;&amp;</span> <span class="nb">sudo chown</span> <span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>:<span class="si">$(</span><span class="nb">id</span> <span class="nt">-g</span><span class="si">)</span> <span class="nv">$HOME</span>/admin.conf <span class="o">&amp;&amp;</span> <span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$HOME</span>/admin.conf
</code></pre></div>    </div>
  </li>
</ul>

<p>Then you can try again.</p>

<p>In the future, if you restart your master node, you may need to also restart the Kubernetes by running</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl restart kubelet.service
</code></pre></div></div>

<p><img src="/images/academic_computing.png" alt="_config.yml" /></p>
:ET